{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f2ffa0a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 17:56:58,246 - d:\\TODO\\Ingeniería\\Ingeniería de Sistemas U. de A\\PoliTo\\SEMESTRES\\Segundo año\\Deep Natural Language Processing\\Project\\chronos-forecasting\\scripts\\training\\train.py - INFO - TF32 format is only available on devices with compute capability >= 8. Setting tf32 to False.\n",
      "2025-06-23 17:56:58,246 - d:\\TODO\\Ingeniería\\Ingeniería de Sistemas U. de A\\PoliTo\\SEMESTRES\\Segundo año\\Deep Natural Language Processing\\Project\\chronos-forecasting\\scripts\\training\\train.py - INFO - Using SEED: 3628479323\n",
      "2025-06-23 17:56:58,259 - d:\\TODO\\Ingeniería\\Ingeniería de Sistemas U. de A\\PoliTo\\SEMESTRES\\Segundo año\\Deep Natural Language Processing\\Project\\chronos-forecasting\\scripts\\training\\train.py - INFO - Logging dir: output\\run-0\n",
      "2025-06-23 17:56:58,259 - d:\\TODO\\Ingeniería\\Ingeniería de Sistemas U. de A\\PoliTo\\SEMESTRES\\Segundo año\\Deep Natural Language Processing\\Project\\chronos-forecasting\\scripts\\training\\train.py - INFO - Loading and filtering 1 datasets for training: ['kernelsynth-data.arrow']\n",
      "2025-06-23 17:56:58,260 - d:\\TODO\\Ingeniería\\Ingeniería de Sistemas U. de A\\PoliTo\\SEMESTRES\\Segundo año\\Deep Natural Language Processing\\Project\\chronos-forecasting\\scripts\\training\\train.py - INFO - Mixing probabilities: [1.0]\n",
      "2025-06-23 17:56:58,263 - d:\\TODO\\Ingeniería\\Ingeniería de Sistemas U. de A\\PoliTo\\SEMESTRES\\Segundo año\\Deep Natural Language Processing\\Project\\chronos-forecasting\\scripts\\training\\train.py - INFO - Initializing model\n",
      "2025-06-23 17:56:58,263 - d:\\TODO\\Ingeniería\\Ingeniería de Sistemas U. de A\\PoliTo\\SEMESTRES\\Segundo año\\Deep Natural Language Processing\\Project\\chronos-forecasting\\scripts\\training\\train.py - INFO - Using pretrained initialization from amazon/chronos-t5-small\n",
      "2025-06-23 17:56:58,263 - d:\\TODO\\Ingeniería\\Ingeniería de Sistemas U. de A\\PoliTo\\SEMESTRES\\Segundo año\\Deep Natural Language Processing\\Project\\chronos-forecasting\\scripts\\training\\train.py - INFO - Using LoRA for fine-tuning\n",
      "The speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "2025-06-23 17:56:59,988 - d:\\TODO\\Ingeniería\\Ingeniería de Sistemas U. de A\\PoliTo\\SEMESTRES\\Segundo año\\Deep Natural Language Processing\\Project\\chronos-forecasting\\scripts\\training\\train.py - INFO - Training\n",
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]W0623 17:59:49.025000 23476 site-packages\\torch\\_inductor\\utils.py:1250] [6/0_1] Not enough SMs to use max_autotune_gemm mode\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"d:\\TODO\\Ingeniería\\Ingeniería de Sistemas U. de A\\PoliTo\\SEMESTRES\\Segundo año\\Deep Natural Language Processing\\Project\\chronos-forecasting\\scripts\\training\\train.py\"\u001b[0m, line \u001b[35m725\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[31mapp\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\typer\\main.py\"\u001b[0m, line \u001b[35m340\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    raise e\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\typer\\main.py\"\u001b[0m, line \u001b[35m323\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mget_command(self)\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\click\\core.py\"\u001b[0m, line \u001b[35m1161\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return \u001b[31mself.main\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\typer\\core.py\"\u001b[0m, line \u001b[35m677\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    return _main(\n",
      "        self,\n",
      "    ...<6 lines>...\n",
      "        **extra,\n",
      "    )\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\typer\\core.py\"\u001b[0m, line \u001b[35m195\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    rv = self.invoke(ctx)\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\click\\core.py\"\u001b[0m, line \u001b[35m1443\u001b[0m, in \u001b[35minvoke\u001b[0m\n",
      "    return \u001b[31mctx.invoke\u001b[0m\u001b[1;31m(self.callback, **ctx.params)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\click\\core.py\"\u001b[0m, line \u001b[35m788\u001b[0m, in \u001b[35minvoke\u001b[0m\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\typer\\main.py\"\u001b[0m, line \u001b[35m698\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return callback(**use_params)\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\typer_config\\decorators.py\"\u001b[0m, line \u001b[35m96\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return cmd(*args, **kwargs)\n",
      "  File \u001b[35m\"d:\\TODO\\Ingeniería\\Ingeniería de Sistemas U. de A\\PoliTo\\SEMESTRES\\Segundo año\\Deep Natural Language Processing\\Project\\chronos-forecasting\\scripts\\training\\train.py\"\u001b[0m, line \u001b[35m712\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mtrainer.train\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\transformers\\trainer.py\"\u001b[0m, line \u001b[35m2245\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "    return inner_training_loop(\n",
      "        args=args,\n",
      "    ...<2 lines>...\n",
      "        ignore_keys_for_eval=ignore_keys_for_eval,\n",
      "    )\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\transformers\\trainer.py\"\u001b[0m, line \u001b[35m2560\u001b[0m, in \u001b[35m_inner_training_loop\u001b[0m\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\transformers\\trainer.py\"\u001b[0m, line \u001b[35m3736\u001b[0m, in \u001b[35mtraining_step\u001b[0m\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\transformers\\trainer.py\"\u001b[0m, line \u001b[35m3801\u001b[0m, in \u001b[35mcompute_loss\u001b[0m\n",
      "    outputs = model(**inputs)\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\torch\\nn\\modules\\module.py\"\u001b[0m, line \u001b[35m1751\u001b[0m, in \u001b[35m_wrapped_call_impl\u001b[0m\n",
      "    return \u001b[31mself._call_impl\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\torch\\nn\\modules\\module.py\"\u001b[0m, line \u001b[35m1762\u001b[0m, in \u001b[35m_call_impl\u001b[0m\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\"\u001b[0m, line \u001b[35m663\u001b[0m, in \u001b[35m_fn\u001b[0m\n",
      "    \u001b[1;31mraise e.remove_dynamo_frames() from None\u001b[0m  # see TORCHDYNAMO_VERBOSE=1\n",
      "    \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"d:\\apps\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\"\u001b[0m, line \u001b[35m3957\u001b[0m, in \u001b[35mcreate_backend\u001b[0m\n",
      "    raise TritonMissing(inspect.currentframe())\n",
      "\u001b[1;35mtorch._inductor.exc.TritonMissing\u001b[0m: \u001b[35mCannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n",
      "\n",
      "Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
      "\u001b[0m\n",
      "\n",
      "  0%|          | 0/1000 [02:49<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 46,449,152 || trainable%: 0.6349\n",
      "THIS IS AMAZING: None\n"
     ]
    }
   ],
   "source": [
    "!python training/train.py --config ./training/configs/chronos-t5-small-lora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd4f088",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1c1f5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
